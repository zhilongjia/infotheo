% Generated by roxygen2 (4.0.2): do not edit by hand
\name{nMI}
\alias{nMI}
\title{normalised mutual information computation}
\usage{
nMI(X, Y = NULL, method = "emp", type = "max.marginal")
}
\arguments{
\item{X}{vector/factor denoting a random variable or a data.frame denoting a random vector where columns contain variables/features and rows contain outcomes/samples.}

\item{Y}{another random variable or random vector (vector/factor or data.frame).}

\item{method}{The name of the entropy estimator. The package implements four estimators : "emp", "mm", "shrink", "sg" (default:"emp") - see details. These estimators require discrete data values - see \code{\link{discretize}}.}

\item{type}{method of normalization. Default is "NULL" and the Mutual Information is calculated as MI = Hx+Hy-Hxy. Other methods include "MI", "marginal", "joint", "min.marginal", "max.marginal", "min.conditional", "max.conditional". See details below.}
}
\value{
\code{nMI} returns the normalised mutual information I(X;Y) in nats.
}
\description{
\code{nMI} takes two random variables as input and computes the mutual
information in nats according to the entropy estimator \code{method}.If Y is
not supplied and X is a matrix-like argument, the function returns a matrix
of mutual information between all pairs of variables in the dataset X.
}
\details{
method of entropy estimator:
\itemize{
\item "emp" : This estimator computes the entropy of the empirical probability distribution.
\item "mm" : This is the Miller-Madow asymptotic bias corrected empirical estimator.
\item "shrink" : This is a shrinkage estimate of the entropy of a Dirichlet probability distribution.
\item "sg" : This is the Schurmann-Grassberger estimate of the entropy of a Dirichlet probability distribution.
}
Type of Normalization:
\itemize{
\item marginal MI = 2*( Hx + Hy - Hxy ) / ( Hx + Hy )
\item joint     MI = 2*( Hx + Hy - Hxy ) / ( Hxy )
\item min.marginal	MI = ( Hx + Hy - Hxy ) / min(Hx,Hy)
\item max.marginal MI = ( Hx + Hy - Hxy ) / max(Hx,Hy)
\item min.conditional MI = ( Hx + Hy - Hxy ) / min(Hx.y,Hy.x)
\item max.conditional MI = ( Hx + Hy - Hxy ) / max(Hx.y,Hy.x)
\item MI MI = ( Hx + Hy - Hxy )
}
}
\examples{
data(USArrests)
dat<-discretize(USArrests)

I <- nMI(dat, method= "emp", type="max.marginal")
I2<- nMI(dat[,1],dat[,2])
}
\author{
Zhilong JIA
}
\references{
{Meyer,  P. E.  (2008). Information-Theoretic Variable Selection and Network Inference from Microarray Data. PhD thesis of the Universite Libre de Bruxelles.}

{Cover, T. M. and Thomas, J. A. (1990). Elements of Information Theory. John Wiley, New York.}

{\code{\link[HDMD]{NMI}}}
}
\seealso{
\code{\link{mulinformation}}, \code{\link[HDMD]{NMI}}
}

